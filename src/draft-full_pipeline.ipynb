{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88117ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/arthur/Documents/Universite/M2-QEA/Machine Learning/final_project/ml_project\n",
      "✓ Datasets loaded: main_df(50044, 10), sport_df(6460, 2), job_df(19336, 11), job_security_df(24224, 2), retired_former_df(13176, 4), retired_jobs_df(11226, 11), retired_pension_df(11226, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set project root to ml_project directory\n",
    "if os.getcwd().endswith('src'):\n",
    "    os.chdir('..')\n",
    "    \n",
    "project_root = os.getcwd()\n",
    "print(f\"Working directory: {project_root}\")\n",
    "\n",
    "# Load datasets\n",
    "main_df = pd.read_csv(\"data/learn_dataset.csv\")\n",
    "sport_df = pd.read_csv(\"data/learn_dataset_sport.csv\")\n",
    "job_df = pd.read_csv(\"data/learn_dataset_job.csv\")\n",
    "job_security_df = pd.read_csv(\"data/learn_dataset_JOB_SECURITY.csv\")\n",
    "retired_former_df = pd.read_csv(\"data/learn_dataset_retired_former.csv\")\n",
    "retired_jobs_df = pd.read_csv(\"data/learn_dataset_retired_jobs.csv\")\n",
    "retired_pension_df = pd.read_csv(\"data/learn_dataset_retired_pension.csv\")\n",
    "job_desc_map_df = pd.read_csv(\"data/code_job_desc_map.csv\")\n",
    "departments_df = pd.read_csv(\"data/departments.csv\")\n",
    "sports_desc_df = pd.read_csv(\"data/code_Sports.csv\")\n",
    "city_pop_df = pd.read_csv(\"data/city_pop.csv\")\n",
    "# city_revenue_df = pd.read_csv(\"data/city_revenue.csv\", sep=';')\n",
    "\n",
    "print(f\"✓ Datasets loaded: main_df{main_df.shape}, sport_df{sport_df.shape}, job_df{job_df.shape}, job_security_df{job_security_df.shape}, retired_former_df{retired_former_df.shape}, retired_jobs_df{retired_jobs_df.shape}, retired_pension_df{retired_pension_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23bedcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Merged datasets with meaningful suffixes:\n",
      "  - Current job columns: 13 (suffix: _current)\n",
      "  - Retired job columns: 13 (suffix: _retired)\n",
      "  - Pension columns: 1\n",
      "  - Sport columns: 1\n"
     ]
    }
   ],
   "source": [
    "job_df = job_df.merge(job_desc_map_df, left_on='job_desc', right_on='N3', how='left')\n",
    "retired_jobs_df = retired_jobs_df.merge(job_desc_map_df, left_on='job_desc', right_on='N3', how='left')\n",
    "\n",
    "# Rename columns with meaningful suffixes before merging\n",
    "job_df_renamed = job_df.rename(columns={col: f\"{col}_current\" for col in job_df.columns if col != 'primary_key'})\n",
    "retired_jobs_df_renamed = retired_jobs_df.rename(columns={col: f\"{col}_retired\" for col in retired_jobs_df.columns if col != 'primary_key'})\n",
    "\n",
    "# Merge datasets\n",
    "df = main_df.merge(job_df_renamed, on='primary_key', how='left')\n",
    "df = df.merge(retired_jobs_df_renamed, on='primary_key', how='left')\n",
    "df = df.merge(retired_pension_df, on='primary_key', how='left')\n",
    "df = df.merge(sport_df, on='primary_key', how='left')\n",
    "df['department'] = df['Insee_code'].str[:2]\n",
    "\n",
    "# Save column lists after merge for use in imputers\n",
    "job_cols_in_df = [col for col in df.columns if col.endswith('_current')]\n",
    "retired_cols_in_df = [col for col in df.columns if col.endswith('_retired')]\n",
    "pension_cols_in_df = [col for col in retired_pension_df.columns if col != 'primary_key' and col in df.columns]\n",
    "sport_cols_in_df = [col for col in sport_df.columns if col != 'primary_key' and col in df.columns]\n",
    "\n",
    "print(f\"✓ Merged datasets with meaningful suffixes:\")\n",
    "print(f\"  - Current job columns: {len(job_cols_in_df)} (suffix: _current)\")\n",
    "print(f\"  - Retired job columns: {len(retired_cols_in_df)} (suffix: _retired)\")\n",
    "print(f\"  - Pension columns: {len(pension_cols_in_df)}\")\n",
    "print(f\"  - Sport columns: {len(sport_cols_in_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7d35d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial numeric features: 5\n",
      "Initial categorical features: 31\n",
      "\n",
      "✓ Categorical features kept (< 100 categories): 26\n",
      "✗ Categorical features dropped (≥ 100 categories): 5\n",
      "\n",
      "Dropped features:\n",
      "  - Insee_code: 13640 unique values\n",
      "  - job_desc_current: 406 unique values\n",
      "  - N3_current: 406 unique values\n",
      "  - job_desc_retired: 394 unique values\n",
      "  - N3_retired: 394 unique values\n",
      "\n",
      "✓ Pipeline created with:\n",
      "  1. Activity-type based imputation\n",
      "     - Categorical columns → 'not_applicable' for non-applicable cases\n",
      "     - Numeric columns (hours/earnings) → 0 for non-applicable cases\n",
      "     - Pension income → 0 for non-retired individuals\n",
      "  2. Sport columns imputation\n",
      "     - All sport columns → 'not_applicable' for missing values\n",
      "  3. Standard preprocessing (imputation, scaling, encoding)\n",
      "  4. One-hot encoding only for features with < 100 categories\n",
      "  5. HistGradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/04p3g2s921j6d8fvnrbb7rcm0000gn/T/ipykernel_31402/2825931033.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna('not_applicable', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Preprocessed data shape: (50044, 782)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer for activity_type based imputation\n",
    "class ActivityTypeImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Impute job and retired columns with 'not_applicable' based on activity_type\"\"\"\n",
    "    \n",
    "    def __init__(self, retired_cols=None, job_cols=None, pension_cols=None):\n",
    "        self.retired_cols = retired_cols\n",
    "        self.job_cols = job_cols\n",
    "        self.pension_cols = pension_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Store the retired columns if not provided\n",
    "        if self.retired_cols is None:\n",
    "            self.retired_cols_ = [col for col in retired_jobs_df.columns if col != 'primary_key']\n",
    "        else:\n",
    "            self.retired_cols_ = self.retired_cols\n",
    "        \n",
    "        # Store the job columns if not provided\n",
    "        if self.job_cols is None:\n",
    "            self.job_cols_ = [col for col in job_df.columns if col != 'primary_key']\n",
    "        else:\n",
    "            self.job_cols_ = self.job_cols\n",
    "        \n",
    "        # Store the pension columns if not provided\n",
    "        if self.pension_cols is None:\n",
    "            self.pension_cols_ = [col for col in retired_pension_df.columns if col != 'primary_key']\n",
    "        else:\n",
    "            self.pension_cols_ = self.pension_cols\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Check if activity_type exists\n",
    "        if 'activity_type' in X_copy.columns:\n",
    "            # Mask for non-retired individuals (for retired job columns)\n",
    "            non_retired_mask = X_copy['activity_type'] != 'type2_1'\n",
    "            \n",
    "            # Mask for non-employed individuals (for job columns)\n",
    "            # Not employed = unemployed (type1_2) or any inactive (type2_X)\n",
    "            non_employed_mask = (X_copy['activity_type'] == 'type1_2') | (X_copy['activity_type'].str.startswith('type2_'))\n",
    "            \n",
    "            # Fill retired job columns for non-retired people\n",
    "            for col in self.retired_cols_:\n",
    "                if col in X_copy.columns:\n",
    "                    if X_copy[col].dtype == 'object' or X_copy[col].dtype.name == 'category':\n",
    "                        # Categorical: fill with 'not_applicable'\n",
    "                        X_copy.loc[non_retired_mask & X_copy[col].isna(), col] = 'not_applicable'\n",
    "                    else:\n",
    "                        # Numeric (hours, earnings): fill with 0\n",
    "                        X_copy.loc[non_retired_mask & X_copy[col].isna(), col] = 0\n",
    "            \n",
    "            # Fill job columns for non-employed people (unemployed or inactive)\n",
    "            for col in self.job_cols_:\n",
    "                if col in X_copy.columns:\n",
    "                    if X_copy[col].dtype == 'object' or X_copy[col].dtype.name == 'category':\n",
    "                        # Categorical: fill with 'not_applicable'\n",
    "                        X_copy.loc[non_employed_mask & X_copy[col].isna(), col] = 'not_applicable'\n",
    "                    else:\n",
    "                        # Numeric (hours, earnings): fill with 0\n",
    "                        X_copy.loc[non_employed_mask & X_copy[col].isna(), col] = 0\n",
    "            \n",
    "            # Fill pension columns with 0 for non-retired people\n",
    "            for col in self.pension_cols_:\n",
    "                if col in X_copy.columns:\n",
    "                    # Pension columns are numeric (income), fill with 0 for non-retired\n",
    "                    X_copy.loc[non_retired_mask & X_copy[col].isna(), col] = 0\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# Custom transformer for sport columns imputation\n",
    "class SportImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Impute sport columns with 'not_applicable' for all missing values\"\"\"\n",
    "    \n",
    "    def __init__(self, sport_cols=None):\n",
    "        self.sport_cols = sport_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Store the sport columns if not provided\n",
    "        if self.sport_cols is None:\n",
    "            self.sport_cols_ = [col for col in sport_df.columns if col != 'primary_key']\n",
    "        else:\n",
    "            self.sport_cols_ = self.sport_cols\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Fill all sport columns with 'not_applicable' for missing values\n",
    "        for col in self.sport_cols_:\n",
    "            if col in X_copy.columns:\n",
    "                # Fill missing values with 'not_applicable'\n",
    "                X_copy[col].fillna('not_applicable', inplace=True)\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# Create custom transformer instances using saved column lists from merge\n",
    "activity_imputer = ActivityTypeImputer(\n",
    "    retired_cols=retired_cols_in_df,\n",
    "    job_cols=job_cols_in_df,\n",
    "    pension_cols=pension_cols_in_df\n",
    ")\n",
    "\n",
    "sport_imputer = SportImputer(\n",
    "    sport_cols=sport_cols_in_df\n",
    ")\n",
    "\n",
    "# Identify column types (after activity imputation will be applied)\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove target and primary_key from features\n",
    "if 'target' in numeric_features:\n",
    "    numeric_features.remove('target')\n",
    "if 'primary_key' in numeric_features:\n",
    "    numeric_features.remove('primary_key')\n",
    "if 'primary_key' in categorical_features:\n",
    "    categorical_features.remove('primary_key')\n",
    "\n",
    "print(f\"Initial numeric features: {len(numeric_features)}\")\n",
    "print(f\"Initial categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Filter categorical features: keep only those with < 100 unique categories\n",
    "categorical_features_filtered = []\n",
    "categorical_features_dropped = []\n",
    "max_categories = 100\n",
    "\n",
    "for col in categorical_features:\n",
    "    n_unique = df[col].nunique()\n",
    "    if n_unique < max_categories:\n",
    "        categorical_features_filtered.append(col)\n",
    "    else:\n",
    "        categorical_features_dropped.append((col, n_unique))\n",
    "\n",
    "print(f\"\\n✓ Categorical features kept (< {max_categories} categories): {len(categorical_features_filtered)}\")\n",
    "print(f\"✗ Categorical features dropped (≥ {max_categories} categories): {len(categorical_features_dropped)}\")\n",
    "\n",
    "if categorical_features_dropped:\n",
    "    print(\"\\nDropped features:\")\n",
    "    for col, n_unique in categorical_features_dropped:\n",
    "        print(f\"  - {col}: {n_unique} unique values\")\n",
    "\n",
    "# Update categorical features list\n",
    "categorical_features = categorical_features_filtered\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create full pipeline with custom transformers, preprocessing, and model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('activity_imputer', activity_imputer),  # Activity-type based imputation\n",
    "    ('sport_imputer', sport_imputer),         # Sport columns imputation\n",
    "    ('preprocessor', preprocessor),           # Standard preprocessing (impute, scale, encode)\n",
    "    ('regressor', HistGradientBoostingRegressor(random_state=42, max_iter=200))\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Pipeline created with:\")\n",
    "print(\"  1. Activity-type based imputation\")\n",
    "print(\"     - Categorical columns → 'not_applicable' for non-applicable cases\")\n",
    "print(\"     - Numeric columns (hours/earnings) → 0 for non-applicable cases\")\n",
    "print(\"     - Pension income → 0 for non-retired individuals\")\n",
    "print(\"  2. Sport columns imputation\")\n",
    "print(\"     - All sport columns → 'not_applicable' for missing values\")\n",
    "print(\"  3. Standard preprocessing (imputation, scaling, encoding)\")\n",
    "print(f\"  4. One-hot encoding only for features with < {max_categories} categories\")\n",
    "print(\"  5. HistGradientBoostingRegressor\")\n",
    "\n",
    "# For saving preprocessed data, we need to fit_transform the preprocessing steps\n",
    "X = df.drop(columns=['target', 'primary_key'], errors='ignore')\n",
    "y = df['target'] if 'target' in df.columns else None\n",
    "\n",
    "# Apply custom imputation first\n",
    "X_imputed = activity_imputer.fit_transform(X)\n",
    "X_imputed = sport_imputer.fit_transform(X_imputed)\n",
    "\n",
    "# Then apply preprocessing\n",
    "main_df_dummies = preprocessor.fit_transform(X_imputed)\n",
    "main_df_dummies = pd.DataFrame(\n",
    "    main_df_dummies,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Add target back if it exists\n",
    "if y is not None:\n",
    "    main_df_dummies['target'] = y.values\n",
    "\n",
    "print(f\"\\n✓ Preprocessed data shape: {main_df_dummies.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "137333ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retired people with both residence info: 10,894\n",
      "Retired people who moved departments: 10,734\n",
      "Share who moved departments: 98.53%\n"
     ]
    }
   ],
   "source": [
    "# Calculate share of retired people who moved departments between job and retirement\n",
    "# Previous_dep_retired: where they lived during their last job\n",
    "# department: where they currently live (from Insee_code)\n",
    "people_with_both = df[['department', 'Previous_dep_retired']].dropna()\n",
    "\n",
    "if len(people_with_both) > 0:\n",
    "    moved_departments = (people_with_both['department'] != people_with_both['Previous_dep_retired']).sum()\n",
    "    share_moved = moved_departments / len(people_with_both) * 100\n",
    "    \n",
    "    print(f\"Retired people with both residence info: {len(people_with_both):,}\")\n",
    "    print(f\"Retired people who moved departments: {moved_departments:,}\")\n",
    "    print(f\"Share who moved departments: {share_moved:.2f}%\")\n",
    "else:\n",
    "    print(\"No retired people with both previous and current department information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e20d03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING HISTGRADIENTBOOSTING MODEL\n",
      "============================================================\n",
      "Training set size: 40035\n",
      "Test set size: 10009\n",
      "\n",
      "Fitting pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/04p3g2s921j6d8fvnrbb7rcm0000gn/T/ipykernel_31402/2825931033.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna('not_applicable', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model trained successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/04p3g2s921j6d8fvnrbb7rcm0000gn/T/ipykernel_31402/2825931033.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna('not_applicable', inplace=True)\n",
      "/var/folders/ph/04p3g2s921j6d8fvnrbb7rcm0000gn/T/ipykernel_31402/2825931033.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna('not_applicable', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Metric                         Train           Test           \n",
      "------------------------------------------------------------\n",
      "R² Score                       0.8227          0.7982         \n",
      "Mean Squared Error             0.0166          0.0189         \n",
      "Root Mean Squared Error        0.1290          0.1376         \n",
      "Mean Absolute Error            0.0973          0.1032         \n",
      "============================================================\n",
      "\n",
      "✓ Good generalization (train R² - test R² = 0.0244)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Prepare data for training\n",
    "X = df.drop(columns=['target', 'primary_key'], errors='ignore')\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING HISTGRADIENTBOOSTING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Fit the full pipeline (includes preprocessing and model)\n",
    "print(\"\\nFitting pipeline...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "print(\"✓ Model trained successfully\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = full_pipeline.predict(X_train)\n",
    "y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<30} {'Train':<15} {'Test':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'R² Score':<30} {train_r2:<15.4f} {test_r2:<15.4f}\")\n",
    "print(f\"{'Mean Squared Error':<30} {train_mse:<15.4f} {test_mse:<15.4f}\")\n",
    "print(f\"{'Root Mean Squared Error':<30} {train_rmse:<15.4f} {test_rmse:<15.4f}\")\n",
    "print(f\"{'Mean Absolute Error':<30} {train_mae:<15.4f} {test_mae:<15.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for overfitting\n",
    "if train_r2 - test_r2 > 0.1:\n",
    "    print(f\"\\n⚠️  Warning: Possible overfitting detected (train R² - test R² = {train_r2 - test_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n✓ Good generalization (train R² - test R² = {train_r2 - test_r2:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dauphine-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
